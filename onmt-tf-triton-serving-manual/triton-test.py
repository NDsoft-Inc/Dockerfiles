import tritonclient.http as httpclient
import numpy as np
import sentencepiece as spm
import os


class SentencePieceTranslator:
    def __init__(self, triton_url="localhost:8000", model_name=None, sp_model_path=None):
        self.triton_client = httpclient.InferenceServerClient(url=triton_url)
        self.model_name = model_name

        # SentencePiece ëª¨ë¸ ë¡œë“œ
        self.sp = None
        if sp_model_path and os.path.exists(sp_model_path):
            self.sp = spm.SentencePieceProcessor()
            self.sp.load(sp_model_path)
            print(f"âœ… SentencePiece ëª¨ë¸ ë¡œë“œë¨: {sp_model_path}")
        else:
            print("âš ï¸  SentencePiece ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ í† í°í™” ì‚¬ìš©.")

    def encode_with_sentencepiece(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ì¸ì½”ë”©"""
        if self.sp:
            token_pieces = self.sp.encode(text, out_type=str)
            print(f"í† í°: {token_pieces}")
            return token_pieces
        else:
            # ìˆ˜ë™ í† í°í™”
            tokens = ['â–' + word for word in text.split()]
            return tokens

    def decode_with_sentencepiece(self, token_pieces):
        """í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©"""
        if self.sp and token_pieces:
            try:
                return self.sp.decode(token_pieces)
            except:
                return ' '.join(token_pieces).replace('â–', ' ').strip()
        else:
            return ' '.join(token_pieces).replace('â–', ' ').strip()

    def translate_korean_to_english(self, korean_text):
        """í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
        try:
            print(f"ğŸ‡°ğŸ‡· ì…ë ¥: {korean_text}")

            # í† í°í™”
            token_pieces = self.encode_with_sentencepiece(korean_text)
            tokens_bytes = [piece.encode('utf-8') for piece in token_pieces]
            tokens_array = np.array([tokens_bytes], dtype=object)
            length_array = np.array([[len(token_pieces)]], dtype=np.int32)

            # Triton ì…ë ¥ ì¤€ë¹„
            inputs = [
                httpclient.InferInput("tokens", tokens_array.shape, datatype="BYTES"),
                httpclient.InferInput("length", length_array.shape, datatype="INT32")
            ]
            inputs[0].set_data_from_numpy(tokens_array, binary_data=True)
            inputs[1].set_data_from_numpy(length_array, binary_data=True)

            # ì¶œë ¥ ì„¤ì •
            outputs = [
                httpclient.InferRequestedOutput("tokens", binary_data=True),
                httpclient.InferRequestedOutput("length", binary_data=True),
                httpclient.InferRequestedOutput("log_probs", binary_data=True)
            ]

            # ì¶”ë¡  ìˆ˜í–‰
            results = self.triton_client.infer(
                model_name=self.model_name,
                inputs=inputs,
                outputs=outputs
            )

            # ê²°ê³¼ ì²˜ë¦¬
            output_tokens = results.as_numpy("tokens")
            log_probs = results.as_numpy("log_probs")

            # ì¶œë ¥ í† í° ì¶”ì¶œ ë° ë””ì½”ë”©
            decoded_tokens = self.extract_output_tokens(output_tokens)
            english_text = self.decode_with_sentencepiece(decoded_tokens)

            print(f"ğŸ‡ºğŸ‡¸ ë²ˆì—­: {english_text}")

            return {
                'korean': korean_text,
                'english': english_text,
                'confidence': float(np.exp(log_probs.mean()))
            }

        except Exception as e:
            print(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return None

    def extract_output_tokens(self, output_tokens):
        """ì¶œë ¥ ë°°ì—´ì—ì„œ í† í° ë¬¸ìì—´ ì¶”ì¶œ"""
        tokens = []
        try:
            # ë‹¤ì°¨ì› ë°°ì—´ì„ í‰ë©´í™”í•˜ì—¬ ì²˜ë¦¬
            flat_tokens = output_tokens.flatten()
            for token in flat_tokens:
                if isinstance(token, bytes) and token:
                    try:
                        decoded = token.decode('utf-8')
                        if decoded and decoded not in ['<s>', '</s>', '<pad>', '<unk>', '']:
                            tokens.append(decoded)
                    except:
                        continue
        except Exception as e:
            print(f"í† í° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return tokens


def get_user_inputs():
    """ì‚¬ìš©ìë¡œë¶€í„° ì„¤ì •ê°’ ì…ë ¥ë°›ê¸°"""
    print("=" * 50)
    print("ğŸš€ SentencePiece Translator ì„¤ì •")
    print("=" * 50)

    # Triton ì„œë²„ URL
    triton_url = "61.252.58.171:18000"

    # ëª¨ë¸ ì´ë¦„
    model_name = input("Triton ëª¨ë¸ ì´ë¦„: ").strip()
    if not model_name:
        print("âŒ ëª¨ë¸ ì´ë¦„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤!")
        return None, None, None

    # SentencePiece ëª¨ë¸ ê²½ë¡œ
    sp_model_path = input("SentencePiece ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.model): ").strip()
    if not sp_model_path:
        sp_model_path = None

    return triton_url, model_name, sp_model_path


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    triton_url, model_name, sp_model_path = get_user_inputs()
    if not model_name:
        return

    # ë²ˆì—­ê¸° ì´ˆê¸°í™”
    translator = SentencePieceTranslator(
        #triton_url=triton_url,
        model_name=model_name,
        sp_model_path=sp_model_path
    )

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_sentences = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤",
        "ì €ëŠ” í•œêµ­ ì‚¬ëŒì…ë‹ˆë‹¤",
        "ê°ì‚¬í•©ë‹ˆë‹¤"
    ]

    print("\n" + "=" * 50)
    print("ğŸš€ í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    successful = 0
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}/{len(test_sentences)}: {sentence}")
        print("-" * 30)

        result = translator.translate_korean_to_english(sentence)
        if result:
            successful += 1
            print(f"âœ… ì„±ê³µ! ì‹ ë¢°ë„: {result['confidence']:.4f}")
        else:
            print("âŒ ì‹¤íŒ¨")

    print(f"\nğŸ¯ ì„±ê³µë¥ : {successful}/{len(test_sentences)} ({successful / len(test_sentences) * 100:.1f}%)")


if __name__ == "__main__":
    main()