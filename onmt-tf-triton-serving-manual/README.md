# ONMT-tf Inference by Triton Inference Server

## Notice
- Last update: 11.11(Tue) 2025 by Dongik - Update dockerfile, now you can just run an image to serve the model.
- Update: 10.10(Fri) 2025 by Dongik.

## About Triton
$NVIDIA\;Triton^{TM}$ Inference Server ëŠ” ëª¨ë¸ ë°°í¬ ë° ì‹¤í–‰ì„ í‘œì¤€í™”í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ê³ , í”„ë¡œë•ì…˜ í™˜ê²½ì— ë¹ ë¥´ê³  í™•ì¥ ê°€ëŠ¥í•œ AIë¥¼ ì œê³µí•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ì¶”ë¡  ì§€ì› ì†Œí”„íŠ¸ì›¨ì–´. It supports various framework suchas TF, Pytorch, onnx and TensorRT.

## Requirements

- ONMT-tf model parameters
- Tokenizer (sentencpiece model)
- Docker image: `nvcr.io/nvidia/tritonserver:23.10-py3`

## Build a docker image
Run command below to make an image.
```
docker build -t deploy/opennmt-tf/tritonserver:23.10-py3 .
```

## Run a container to serve and test ONMT-tf
Run commands to serve a model.

```
docker run --rm -it --gpus "device=4" --name <container_name> --shm-size=256m -v <host_mount_path_to_params>:/models deploy/opennmt-tf/tritonserver:23.10-py3

(e.g.) docker run --rm -it --gpus "device=4" --name sdi_deploy_gpu-4 --shm-size=256m -v /home/sdi/onmt-tf/komy/triton/:/models deploy/opennmt-tf/tritonserver:23.10-py3
```

If you see the status below, the model is ready to respond

```
+---------------+---------+--------+  
| ModelÂ Â Â Â Â Â Â Â  | Version | Status |  
+---------------+---------+--------+  
| koenÂ Â Â Â Â Â Â Â Â Â | 1Â Â Â Â Â Â Â | READYÂ  |  
| kojaÂ Â Â Â Â Â Â Â Â Â | 1Â Â Â Â Â Â Â | READYÂ  |  
+---------------+---------+--------+
```

To test the model, you can run a script. Open additional terminal

```
docker exec -it <container_name> zsh

python3 triton-test.py
```

Then, you can input a sentencepiece directory.

```
==================================================
ğŸš€ SentencePiece Translator ì„¤ì •
==================================================
Triton ëª¨ë¸ ì´ë¦„: komy
SentencePiece ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.model): /models/komy/wmt.komy.0516.model
```

## (Appendix) Docker option configuration

Start docker container by running following command.
```
docker run --rm -it --gpus "device=4" --name sdi_triton --shm-size=256m -v /home/sdi/onmt-tf/komy/triton:/models nvcr.io/nvidia/tritonserver:23.10-py3 

# if you build your own image run this
(optional) docker run --rm -it --gpus "device=4" --name sdi_triton --shm-size=256m -v /home/sdi/onmt-tf/komy/triton:/models my/tritonserver:23.10-py3 
```
- --rm: ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•˜ê³  í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ë©´ ì¦‰ì‹œ ì‚­ì œ
- --it: í‘œì¤€ ì…ë ¥(stdin) í„°ë¯¸ë„ì—ì„œ ì…ë ¥ì„ ì»¨í…Œì´ë„ˆë¡œ ì „ë‹¬, ê°€ìƒ í„°ë¯¸ë„(tty) ì„ í• ë‹¹, ì‰˜ í™˜ê²½ì²˜ëŸ¼ í‘œì‹œ
- --gpus: ì»¨í…Œì´ë„ˆì— GPU ìì›ì„ í• ë‹¹ (e.g. --gpus "device=0,2")
- --shm-size: ì»¨í…Œì´ë„ˆì˜ ê³µìœ  ë©”ëª¨ë¦¬(shared memory) í¬ê¸°ë¥¼ ì§€ì •
    - ë¦¬ëˆ…ìŠ¤ì—ì„œëŠ” /dev/shm ê²½ë¡œê°€ ê³µìœ  ë©”ëª¨ë¦¬ ê³µê°„
    - Docker ì»¨í…Œì´ë„ˆëŠ” ê¸°ë³¸ì ìœ¼ë¡œ /dev/shm í¬ê¸°ê°€ 64MBë¡œ ì œí•œ
    - ëŒ€ê·œëª¨ ì—°ì‚°(ì˜ˆ: PyTorch DataLoader ë³‘ë ¬ ë¡œë”©, Chrome headless, multiprocessing ë“±)ì„ ì‚¬ìš©í•  ë•Œ ì´ ê³µê°„ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ
- -v: <host_path>:<container_path>[:option]
    - ro: read only
    - rw (default): read and write
    - multiple mount support by calling multiple `-v` options (e.g.) 
        ```
        docker run -it --rm \
            -v $(pwd):/workspace \
            python:3.12 \
            python /workspace/train.py
        ```

Install following packages.    
```
pip install tritonclient[http]
pip install sentencepiece
```